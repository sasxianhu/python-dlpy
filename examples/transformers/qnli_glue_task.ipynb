{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Question Natural Language Inference </h1>\n",
    "\n",
    "This example fine-tunes a BERT model for one of the general language understanding evaluation (GLUE) benchmark tasks.  This benchmark is called question natural language inference (QNLI).  \n",
    "\n",
    "Devlin, Chang, Lee, and Toutanova, authors of [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) define Question Natural Language Inference as follows:\n",
    "\n",
    "\"Question Natural Language Inference is a version of the Stanford Question Answering Dataset (Rajpurkar et al., 2016) which has been converted to a binary classification task (Wang et al., 2018). The positive examples are (question, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer.\"\n",
    "\n",
    "This Jupyter notebook contains an example of a natural language inference task that involves two sentences (question, sentence).  Here, the term *sentence* refers to an arbitrary span of contiguous text, rather than an actual linguistic sentence. \n",
    "\n",
    "Fine-tuning a BERT model for QNLI requires multiple steps. The steps include integrated data preparation and model creation tasks. SAS DLPy provides detailed and documented examples of these steps in the companion tutorial Jupyter notebooks *bert_data_preparation_quickstart* and *bert_load_model_quickstart*. \n",
    "\n",
    "This end-to-end BERT QNLI example follows the sequence of steps shown below: \n",
    "\n",
    "* [Define SAS Viya Version of BERT Model](#Define-Viya-BERT)\n",
    "* [Explore and Clean Training Data](#Data-Exploration-and-Cleaning-Training)\n",
    "* [Tokenize Training Data and Upload as a SAS Viya Table](#Data-Tokenization-Training)\n",
    "* [Compile the BERT Classification Model ](#Compile-BERT-Model)\n",
    "* [Attach BERT Model Parameters](#Attach-BERT-Parms)\n",
    "* [Fine-tune BERT Model for QNLI Task](#Fine-Tune-Train)\n",
    "* [Explore and Clean Test Data](#Data-Exploration-and-Cleaning-Test)\n",
    "* [Tokenize Test Data and Upload as a SAS Viya Table](#Data-Tokenization-Test)\n",
    "* [Perform Inference Using the Test Data](#Inference-Test-Data)\n",
    "\n",
    "The code below navigates each step and concludes by scoring a test data set that demonstrates the effectiveness of the fine-tuned BERT model.\n",
    "\n",
    "<h2> Client and Server Terminology in this Example </h2>\n",
    "\n",
    "SAS Viya literature and technical documentation often refers to client and server entities. In this scenario, the client is the computer that runs the Jupyter notebook. The server is the computer that is running the Viya server. These two computers might (or might not) use the same operating system, and might (or might not) have access to a common file system.\n",
    "\n",
    "This example assumes that the client and server use the same operating system, and that they have access to a common file system. If the client and server in your environment do not have access to a common file system, you will need to copy or transfer files between client and server during this example. This notebook uses comments in cells to indicate when a given file should be moved from the client to the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example begins by configuring the computing environment for the modeling task to follow. \n",
    "\n",
    "Configuring the environments includes [importing required Python packages](#ImportPackages), [specifying client parameters](#Specify-Client-Parms), [connecting to the SAS Viya Server](#Connect-to-SAS-Viya-Server), and [specifying the modeling task parameters](#Set-Example-Parms). \n",
    "\n",
    "<a id=\"ImportPackages\"></a>\n",
    "<h2> Import Python Function Packages </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary Python packages\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Specify-Client-Parms\"></a>\n",
    "<h3> Specify Client Parameters </h3>\n",
    "\n",
    "This section of code configures the notebook for use with Linux or Windows SAS Viya clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize the example Jupyter notebook so it can be used with Linux and Windows Viya clients\n",
    "\n",
    "# Linux client parameters\n",
    "if sys.platform.startswith('linux'):\n",
    "    \n",
    "    # path to HuggingFace cache directory\n",
    "    cache_dir='/path-to/huggingface-cache-dir'  \n",
    "    \n",
    "# Windows client parameters    \n",
    "elif sys.platform.startswith('win'):\n",
    "            \n",
    "    # path to HuggingFace cache directory\n",
    "    cache_dir='Drive:\\\\path-to\\\\huggingface-cache-dir'\n",
    "\n",
    "# Unsupported operating system client    \n",
    "else:\n",
    "    raise ValueError('Unrecognized operating system')\n",
    "    \n",
    "from dlpy.transformers.bert_model import BERT_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Connect-to-SAS-Viya-Server\"></a>\n",
    "<h3> Connect to the SAS Viya Server </h3>\n",
    "\n",
    "This section of code invokes the SAS Wrapper for Analytics Transfer ([SWAT](https://github.com/sassoftware/python-swat)), configures the Python matplotlib utility for Jupyter notebook output display, connects to a SAS Viya server, and loads the SAS CAS DeepLearn action set into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'deepLearn'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; actionset</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>deepLearn</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.00309s</span> &#183; <span class=\"cas-user\">user 0.00307s</span> &#183; <span class=\"cas-memory\">mem 0.205MB</span></small></p>"
      ],
      "text/plain": [
       "[actionset]\n",
       "\n",
       " 'deepLearn'\n",
       "\n",
       "+ Elapsed: 0.00309s, user: 0.00307s, mem: 0.205mb"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The SAS Scripting Wrapper for Analytic Transfer (SWAT)\n",
    "# is a module used to connect to a SAS Viya server.\n",
    "from swat import * \n",
    "\n",
    "# Configure the Python matplotlib utility to display output in \n",
    "# Jupyter notebook cells.\n",
    "%matplotlib inline\n",
    "\n",
    "# Specify the name of your SAS Viya host\n",
    "host = 'your-host-name'\n",
    "\n",
    "# Specify your installation's port ID (5 digit number) and user ID parameters.\n",
    "s = CAS(host, portID, 'userID')\n",
    "\n",
    "# load the SAS Viya Deep Learning Action Sets\n",
    "s.loadactionset('deepLearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Set-Example-Parms\"></a>\n",
    "<h3> Set the Example Task Parameters </h3>\n",
    "\n",
    "The task performed in this Jupyter notebook example is to determine whether the answer to a question is given in the sentence portion of a question/sentence pair.  This is an instance of a binary classification problem where\n",
    "\n",
    "* *class 1*: sentence answers question\n",
    "* *class 2*: sentence does not answer question\n",
    "\n",
    "The variable ``n_classes`` in the cell below indicates that the final classification layer in the BERT model requires predictions for two classes. \n",
    "\n",
    "BERT models are composed of a number of [Transformer](http://jalammar.github.io/illustrated-transformer/) encoder blocks or layers.  For this binary classification task, the BERT<sub>base</sub> model described in the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) is used.  This model consists of twelve encoder layers.  You can choose to incorporate any number of encoder layers up to maximum the model supports.  The variable ``num_encoder_layers`` in the cell below indicates that all twelve layers should be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task parameters for BERT model\n",
    "\n",
    "# Final classification layer number of class predictions\n",
    "n_classes = 2\n",
    "\n",
    "# Number of BERT encoder layers used\n",
    "num_encoder_layers = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Define-Viya-BERT\"></a>\n",
    "<h2> Define SAS Viya Version of BERT Model </h2>\n",
    "\n",
    "First, create an instance of the ``BERT_Model`` class.  This class uses the ``bert-base-uncased`` model from the [HuggingFace](https://huggingface.co/transformers/installation.html) repository to build the SAS Viya BERT model.  The code in the notebook cell below assumes that you have already run the following code to download the necessary JSON files (see [pre-trained models](https://huggingface.co/transformers/pretrained_models.html)) to your local cache directory:\n",
    "\n",
    "\n",
    "``model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                    cache_dir=/path-to/your-cache-directory\n",
    "                                   )``\n",
    "\n",
    "When the ``BERT_Model`` class defines a new BERT model, it also prepares the embedding table that is needed to specify the token, position, and segment inputs for the BERT encoder layers.  \n",
    "\n",
    "In the following code block, note that verbose output is enabled. Verbose output provides additional feedback for the start, progress, and completion of some internal computation operations that are not typically surfaced. The additional process information is useful as a \"heartbeat\" indicator (*the computation is still processing*), because some internal BERT tasks for large models can require large amounts of time to complete.\n",
    "\n",
    "**Note:** The new ``bert`` object is a fully functional SAS DLPy model object.  This means that you can train, score, and evaluate the BERT model just as you would any other DLPy model object.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: loading base BERT model bert-base-uncased ...\n",
      "NOTE: base BERT model loaded.\n",
      "NOTE: loading BERT embedding table ... \n",
      "NOTE: BERT embedding table loaded.\n"
     ]
    }
   ],
   "source": [
    "# instantiate a version of the HuggingFace BERT model\n",
    "huggingface_name = 'bert-base-uncased'\n",
    "\n",
    "bert = BERT_Model(s,\n",
    "                  cache_dir,\n",
    "                  huggingface_name,\n",
    "                  n_classes,\n",
    "                  num_hidden_layers = num_encoder_layers,\n",
    "                  max_seq_len=256,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Download-the-Data\"></a>\n",
    "<h3> Download the Raw Data </h3>\n",
    "\n",
    "In this section, you download the raw data that you will prepare for use with a BERT model.\n",
    "\n",
    "Obtain the QNLI data ([QNLIv2.zip](https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601)) for the example, and unzip to a directory that is accessible from your Viya client. You should end up with three files: ``train.tsv``, ``test.tsv``, and ``dev.tsv``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data-Exploration-and-Cleaning-Training\"></a>\n",
    "<h2>Explore and Clean the Training Data</h2>\n",
    "\n",
    "After downloading and unzipping ``QNLIv2.zip``, you can open ``train.tsv`` and examine the contents. You can use your favorite spreadsheet or text editor to open this file and scan the contents. Each observation contains a (question, sentence) pair, along with the correct label.   \n",
    "\n",
    "The following code reads the ``train.tsv`` data for index, question (input), sentence (input), and label (target) into a Pandas data frame.\n",
    "\n",
    "<a id=\"Read-Data-into-Memory\"></a>\n",
    "<h3>Read the Data into Pandas Data Frame</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training data set for QNLI classification\n",
    "train_data = pd.read_csv('/path-to/qnli-data/train.tsv',\n",
    "                         header=0,\n",
    "                         sep='\\t',\n",
    "                         error_bad_lines=False,\n",
    "                         warn_bad_lines=False,\n",
    "                         names=['index', \n",
    "                                'question',\n",
    "                                'sentence',\n",
    "                                'label'\n",
    "                               ]\n",
    "                         )\n",
    "# Labels for inputs\n",
    "input_a_label = 'question'\n",
    "input_b_label = 'sentence'\n",
    "# Labels for targets\n",
    "target_label = 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Extract Training Data and Assign Numeric Classes </h3>\n",
    "\n",
    "To enable the BERT analysis, the target variable values must be translated from the orignal nominal values (*entailment* and *not_entailment*) to consistent numeric integer values.  The translation uses simple rules:\n",
    "\n",
    "*not_entailment* <=> 1  \n",
    "*entailment* <=> 2\n",
    "\n",
    "You can use a different numeric translation convention if you desire, as long as you are consistent.  The nominal to numeric translation must be performed before the ``bert_prepare_data()`` statement is processed.  \n",
    "\n",
    "**Note**: there are two inputs in the example: *question* and *sentence*.  The inputs are combined in a subsequent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two inputs and the target\n",
    "input_a = train_data[input_a_label].to_list()\n",
    "input_b = train_data[input_b_label].to_list()\n",
    "targets = train_data[target_label].to_list()\n",
    "\n",
    "# Translate nominal target values \n",
    "# to numeric integer values\n",
    "for ii, tgt in enumerate(targets):\n",
    "    if tgt == 'entailment':\n",
    "        targets[ii] = 2\n",
    "    elif tgt == 'not_entailment':\n",
    "        targets[ii] = 1\n",
    "    else:\n",
    "        print('Unknown target label.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data-Tokenization-Training\"></a>\n",
    "<h2> Tokenize the Training Data and Upload as a SAS Viya Table </h2>\n",
    "\n",
    "After you extract the input data, the data must be tokenized for use with BERT.\n",
    "\n",
    "The input data consists of text strings that are composed of English words, but BERT networks cannot operate directly on text data. Instead, the text must be tokenized using the [WordPiece algorithm](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf).  \n",
    "\n",
    "The WordPiece algorithm segments words into representative components, or tokens for NLP tasks. The WordPiece tokens are translated to WordPiece embeddings internally by the BERT model. Embeddings are a vector of numeric values that represent the tokens. BERT combines the WordPiece embeddings with position and segment embeddings when forming the input to the first encoder layer.  \n",
    "\n",
    "A full discussion of these embeddings is beyond the scope of this example, but you can find a tutorial-style explanation [here](http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/).  \n",
    "\n",
    "After the input data has been tokenized, the table is ready to be uploaded as a SAS Viya table.\n",
    "\n",
    "The helper function ``bert_prepare_data()`` handles all of the data preparation and formatting that is required to upload the SAS Viya table, including combining the question/sentence into a single input of the form\n",
    "\n",
    "    [CLS] question tokens [SEP] answer tokens [SEP]\n",
    "\n",
    "The last parameter in the ``bert_prepare_data`` specification requests verbose log output. This setting provides detailed user feedback that you can use to track the progress of the data preparation. Be warned that some large data sets might take a long time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: 10% of the observations tokenized.\n",
      "NOTE: 20% of the observations tokenized.\n",
      "NOTE: 30% of the observations tokenized.\n",
      "NOTE: 40% of the observations tokenized.\n",
      "NOTE: 50% of the observations tokenized.\n",
      "NOTE: 60% of the observations tokenized.\n",
      "NOTE: 70% of the observations tokenized.\n",
      "NOTE: 80% of the observations tokenized.\n",
      "NOTE: 90% of the observations tokenized.\n",
      "NOTE: 100% of the observations tokenized.\n",
      "NOTE: all observations tokenized.\n",
      "\n",
      "WARNING: 122 out of 103106 observations exceeded the maximum sequence length\n",
      "These observations have been truncated so that only the first 256 tokens are used.\n",
      "\n",
      "NOTE: uploading data to table bert_data.\n",
      "NOTE: there are 103106 observations in the data set.\n",
      "\n",
      "NOTE: data set ready.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the DLPy bert_prepare_data function\n",
    "from dlpy.transformers.bert_utils import bert_prepare_data\n",
    "\n",
    "# Specify the bert_prepare_data function parameters\n",
    "num_tgt_var, train = bert_prepare_data(s, \n",
    "                                       bert.get_tokenizer(), \n",
    "                                       bert.get_max_sequence_length(), \n",
    "                                       input_a=input_a, \n",
    "                                       input_b=input_b, \n",
    "                                       target=targets, \n",
    "                                       classification_problem=bert.get_problem_type(), \n",
    "                                       segment_vocab_size=bert.get_segment_size(), \n",
    "                                       verbose=True\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Review and Verify the BERT Data </h2>\n",
    "\n",
    "According to the output, the example should have created a new train data table. Now, review the new table and verify that the new table has the expected number and type of columns, headings, and cell contents. After verifying proper column structures and contents in the new tables, you can also display the summary statistics to get a sense of the data.\n",
    "\n",
    "* [Verify Training Table Columns](#Verify-Train-Columns)\n",
    "* [Review Training Data](#Review-Train-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Verify-Train-Columns\"></a>\n",
    "<h3> Verify the Training Table Columns </h3>\n",
    "\n",
    "The ``bert_prepare_data`` function creates variables in the SAS Viya table that are used by the token, position, and segment embeddings.  If you are using data that has associated labels, the function also creates target and target length variables.  Since this example uses labeled data, the output of the *columinfo* action in the following code should contain at least 5 columns:\n",
    "\n",
    "* \\_tokens\\_         => tokenized input text strings\n",
    "* \\_position\\_       => position indication strings used by position embeddings\n",
    "* \\_segment\\_        => segment indication strings used by segment embeddings\n",
    "* \\_target\\_0\\_      => target variable (only one for this example)\n",
    "* \\_target\\_length\\_ => target length variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ColumnInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Column\">Column</th>\n",
       "      <th title=\"Label\">Label</th>\n",
       "      <th title=\"ID\">ID</th>\n",
       "      <th title=\"Type\">Type</th>\n",
       "      <th title=\"RawLength\">RawLength</th>\n",
       "      <th title=\"FormattedLength\">FormattedLength</th>\n",
       "      <th title=\"Format\">Format</th>\n",
       "      <th title=\"NFL\">NFL</th>\n",
       "      <th title=\"NFD\">NFD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_tokens_</td>\n",
       "      <td>_tokens_</td>\n",
       "      <td>1</td>\n",
       "      <td>varchar</td>\n",
       "      <td>1434</td>\n",
       "      <td>1434</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_position_</td>\n",
       "      <td>_position_</td>\n",
       "      <td>2</td>\n",
       "      <td>varchar</td>\n",
       "      <td>3217</td>\n",
       "      <td>3217</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_segment_</td>\n",
       "      <td>_segment_</td>\n",
       "      <td>3</td>\n",
       "      <td>varchar</td>\n",
       "      <td>2815</td>\n",
       "      <td>2815</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_target_0_</td>\n",
       "      <td>_target_0_</td>\n",
       "      <td>4</td>\n",
       "      <td>varchar</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_target_length_</td>\n",
       "      <td>_target_length_</td>\n",
       "      <td>5</td>\n",
       "      <td>double</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.000646s</span> &#183; <span class=\"cas-user\">user 0.00063s</span> &#183; <span class=\"cas-memory\">mem 0.802MB</span></small></p>"
      ],
      "text/plain": [
       "[ColumnInfo]\n",
       "\n",
       "             Column            Label  ID     Type  RawLength  FormattedLength  \\\n",
       " 0         _tokens_         _tokens_   1  varchar       1434             1434   \n",
       " 1       _position_       _position_   2  varchar       3217             3217   \n",
       " 2        _segment_        _segment_   3  varchar       2815             2815   \n",
       " 3       _target_0_       _target_0_   4  varchar          1                1   \n",
       " 4  _target_length_  _target_length_   5   double          8               12   \n",
       " \n",
       "   Format  NFL  NFD  \n",
       " 0           0    0  \n",
       " 1           0    0  \n",
       " 2           0    0  \n",
       " 3           0    0  \n",
       " 4           0    0  \n",
       "\n",
       "+ Elapsed: 0.000646s, user: 0.00063s, mem: 0.802mb"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display train table column info\n",
    "s.columninfo(table=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Review-Train-Data\"></a>\n",
    "<h3> Review the Training Data </h3>\n",
    "\n",
    "This step is a good development practice, but is not strictly necessary. It is usually a good idea to review your pre-processed data in order to verify that there are no remaining artifacts.  The code and output cell below displays the token and target column values for a small number of random observations, so you can visually inspect them.  You can control what is displayed through the ``num_obs`` and ``columns`` arguments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Observation:  101014 -------\n",
      "\n",
      "_tokens_ :  [CLS] in 1840 , about how many african - americans lived in new york city ? [SEP] under such influential united states founders as alexander hamilton and john jay , the new york man ##umi ##ssion society worked for abolition and established the african free school to educate black children . [SEP]\n",
      "\n",
      "\n",
      "_target_0_ :  1\n",
      "\n",
      "\n",
      "------- Observation:  8736 -------\n",
      "\n",
      "_tokens_ :  [CLS] what part of birds was found on an expedition to mt . everest ? [SEP] an expedition to mt . everest found skeletons of northern pin ##tail ana ##s ac ##uta and black - tailed god ##wi ##t limo ##sa limo ##sa at 5 , 000 m ( 16 , 000 ft ) on the k ##hum ##bu glacier . [SEP]\n",
      "\n",
      "\n",
      "_target_0_ :  2\n",
      "\n",
      "\n",
      "------- Observation:  31988 -------\n",
      "\n",
      "_tokens_ :  [CLS] what is another name for the u ##b ##da ? [SEP] not every person accused of a political crime was convicted and nobody was sentenced to death for his or her pro - soviet feelings . [SEP]\n",
      "\n",
      "\n",
      "_target_0_ :  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dlpy.transformers.bert_utils import display_obs\n",
    "\n",
    "display_obs(s, \n",
    "            train, \n",
    "            num_obs=3, \n",
    "            columns=['_tokens_',\n",
    "                     '_target_0_'\n",
    "                    ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Compile-BERT-Model\"></a>\n",
    "<h2> Compile the BERT Classification Model </h2>\n",
    "\n",
    "We exploit the functional API approach by using SAS DLPy to create a BERT model that terminates with a classification head. This step maps the layer definitions from the HuggingFace BERT model to equivalent SAS Deep Learning layers.  Given these SAS layer definitions, a SAS equivalent BERT model is created and all layers are connected.  \n",
    "\n",
    "**Note:** So far, the example has not assigned any model parameters. Model paramters are assigned after the BERT model is compiled, using the HDF5 file (``bert-base-uncased.kerasmodel.h5``) generated as part of this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: HDF5 file is /path-to/huggingface-cache-dir/bert-base-uncased.kerasmodel.h5\n",
      "NOTE: Model compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "# Compile the BERT HDF5 file\n",
    "bert.compile(num_target_var=num_tgt_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Attach-BERT-Parms\"></a>\n",
    "<h2> Attach the BERT Model Parameters </h2>\n",
    "\n",
    "After compiling the BERT classification model, it is time to attach the base BERT model parameters, as well as the randomly initialized parameters used for the model's classification head.  The code below specifies that the base BERT model layers should not be frozen when training (``freeze_base_model=False``), so all model layers will be updated when fine-tuning the model.  \n",
    "\n",
    "If you choose to specify ``freeze_base_model=True``, then only the final classification layer in the BERT model will be trained.  The parameters for all other layers will be frozen.\n",
    "\n",
    "If your client and server do not share a common file system, you **must** manually copy the file ``bert-base-uncased.kerasmodel.h5`` from your client to your server before you can run the following code block.  You must also provide the fully qualified file name on the server when you invoke the ``load_weights()`` function.  \n",
    "\n",
    "The fully-qualified client file name is displayed in the output cell after a successful BERT model compile. The file name is part of the output message: \n",
    "\n",
    "``NOTE: HDF5 file is /path-to-local-cache-dir/bert-base-uncased.kerasmodel.h5.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Model weights attached successfully!\n"
     ]
    }
   ],
   "source": [
    "# Attach the BERT model parameters \n",
    "bert.load_weights(bert.get_hdf5_file_name(), \n",
    "                  num_target_var=num_tgt_var, \n",
    "                  freeze_base_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Fine-Tune-Train\"></a>\n",
    "<h2> Fine-tune the BERT Model for QNLI Task </h2>\n",
    "\n",
    "The example uses the ``fit()`` member function to fine-tune the BERT model.  The BERT_Model class provides a number of convenience functions that enable you to set the required ``fit()`` parameters automatically:\n",
    "\n",
    "* ``get_data_spec()`` - returns the data specification for input/output layers\n",
    "* ``get_optimizer()`` - returns an Optimizer object suitable for fine-tuning training\n",
    "* ``get_text_parameters()`` - returns a TextParms object needed for input data parsing\n",
    "\n",
    "When you create a BERT model, an instance of an Optimizer object is defined, along with some default training settings.  The default settings were obtained from the **Fine-tuning Procedure** section of [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805). The code below overrides the default learning rate and resets the value to 2x10<sup>-5</sup>.  Since the default mini-batch size per thread is 1, setting the number of threads to 32 ensures that the total mini-batch size is 32.\n",
    "\n",
    "The example code specified verbose output earlier when creating the BERT model, so the training process will also provide detailed information during training initiation and for each mini-batch.  A model summary and training history is also provided after the training completes.  If you don't want this level of detail, simply set ``verbose=False`` when instantiating your BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Training based on existing weights.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 86828546.\n",
      "NOTE:  The approximate memory cost is 20536.00 MB.\n",
      "NOTE:  Loading weights cost       0.48 (s).\n",
      "NOTE:  Initializing each layer cost      11.97 (s).\n",
      "NOTE:  The total number of threads on each worker is 32.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 1.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 32.\n",
      "NOTE:  Target variable: _target_0_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: 1\n",
      "NOTE:  Level      1: 2\n",
      "NOTE:  Number of input variables:     3\n",
      "NOTE:  Number of text input variables:      3\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    32  0.00002           0.7411     0.4688    12.02\n",
      "NOTE:      1    32  0.00002           0.7341        0.5    12.56\n",
      "NOTE:      2    32  0.00002           0.7356        0.5    11.01\n",
      "NOTE:      3    32  0.00002           0.6376     0.2813    12.04\n",
      "                       .\n",
      "                       .\n",
      "                       .\n",
      "NOTE:   3220    32  0.00002           0.1104     0.0625    10.53\n",
      "NOTE:   3221    32  0.00002           0.1038    0.03125    13.55\n",
      "NOTE:   3222    32  0.00002          0.05598    0.03125    10.71\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0          2E-5          0.2352    0.08773 37742.59\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    32  0.00002           0.3749      0.125    11.57\n",
      "NOTE:      1    32  0.00002           0.2231     0.0625    10.76\n",
      "NOTE:      2    32  0.00002           0.1696     0.0625    10.26\n",
      "NOTE:      3    32  0.00002           0.2966      0.125    11.29\n",
      "                       .\n",
      "                       .\n",
      "                       .\n",
      "NOTE:   3220    32  0.00002          0.06306    0.03125    12.18\n",
      "NOTE:   3221    32  0.00002           0.2581    0.09375    11.81\n",
      "NOTE:   3222    32  0.00002           0.1799    0.03125    12.46\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1          2E-5          0.1362    0.04634 37729.33\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    32  0.00002           0.0678    0.03125    13.18\n",
      "NOTE:      1    32  0.00002           0.2914     0.0625    11.57\n",
      "NOTE:      2    32  0.00002          0.05458    0.03125    10.08\n",
      "NOTE:      3    32  0.00002          0.02405          0    11.23\n",
      "                       .\n",
      "                       .\n",
      "                       .\n",
      "NOTE:   3220    32  0.00002         0.009768          0    10.92\n",
      "NOTE:   3221    32  0.00002          0.01084          0    13.07\n",
      "NOTE:   3222    32  0.00002          0.01971          0    11.74\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2          2E-5         0.09493    0.03019 37832.76\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is  113304.68 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Recurrent Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of Recurrent Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of multi-head attention layers</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Number of layer normalization layers</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>86724864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>103682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>86828546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>20558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.235187</td>\n",
       "      <td>0.087729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.136159</td>\n",
       "      <td>0.046337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.094927</td>\n",
       "      <td>0.030193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(docair)</td>\n",
       "      <td>classification_weights</td>\n",
       "      <td>86828546</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('classification_weights', caslib='CAS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 1.13e+05s</span> &#183; <span class=\"cas-user\">user 2.45e+06s</span> &#183; <span class=\"cas-sys\">sys 8.64e+03s</span> &#183; <span class=\"cas-memory\">mem 2.23e+04MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                     Value\n",
       " 0                                  Model Name            classification\n",
       " 1                                  Model Type  Recurrent Neural Network\n",
       " 2                            Number of Layers                        92\n",
       " 3                      Number of Input Layers                         3\n",
       " 4                     Number of Output Layers                         1\n",
       " 5              Number of Convolutional Layers                         0\n",
       " 6                    Number of Pooling Layers                         0\n",
       " 7            Number of Fully Connected Layers                        25\n",
       " 8                  Number of Recurrent Layers                         1\n",
       " 9                   Number of Residual Layers                        25\n",
       " 10      Number of multi-head attention layers                        12\n",
       " 11       Number of layer normalization layers                        25\n",
       " 12                Number of Weight Parameters                  86724864\n",
       " 13                  Number of Bias Parameters                    103682\n",
       " 14           Total Number of Model Parameters                  86828546\n",
       " 15  Approximate Memory Cost for Training (MB)                     20558\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "    Epoch  LearningRate      Loss  FitError\n",
       " 0      1       0.00002  0.235187  0.087729\n",
       " 1      2       0.00002  0.136159  0.046337\n",
       " 2      3       0.00002  0.094927  0.030193\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                    Name      Rows  Columns  \\\n",
       " 0  CASUSER(docair)  classification_weights  86828546        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('classification_weights', caslib='CAS...  \n",
       "\n",
       "+ Elapsed: 1.13e+05s, user: 2.45e+06s, sys: 8.64e+03s, mem: 2.23e+04mb"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Override default learning rate\n",
    "bert.set_optimizer_parameters(learning_rate=2e-5)\n",
    "    \n",
    "# Fine-tune the BERT model\n",
    "bert.fit(train, \n",
    "         data_specs=bert.get_data_spec(num_tgt_var),\n",
    "         optimizer=bert.get_optimizer(), \n",
    "         text_parms=bert.get_text_parameters(), \n",
    "         seed=12345, \n",
    "         n_threads=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data-Exploration-and-Cleaning-Test\"></a>\n",
    "<h2>Explore and Clean the Test Data</h2>\n",
    "\n",
    "This example step uses the **dev** data set to evaluate our trained model.  The **dev** set was originally intended to be used for hyperparameter tuning and validation, but the example uses the **dev** data set to test the trained model because it includes labels. (The **test** data set does not include labels.)\n",
    "\n",
    "<h3> Read the Data Into Pandas Data Frame  </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dev dataset for QNLI classification\n",
    "test_data = pd.read_csv('/path-to/qnli-data/dev.tsv',\n",
    "                         header=0,\n",
    "                         sep='\\t',\n",
    "                         error_bad_lines=False,\n",
    "                         warn_bad_lines=False,\n",
    "                         names=['index', \n",
    "                                'question',\n",
    "                                'sentence',\n",
    "                                'label'\n",
    "                                ]\n",
    "                        )\n",
    "\n",
    "# Labels for the dev inputs and target\n",
    "input_a_label = 'question'\n",
    "input_b_label = 'sentence'\n",
    "target_label = 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Extract the Test data and Assign Numeric Classes </h3>\n",
    "\n",
    "This process follows the same procedure that was used with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for the test inputs and target\n",
    "input_a = test_data[input_a_label].to_list()\n",
    "input_b = test_data[input_b_label].to_list()\n",
    "targets = test_data[target_label].to_list()\n",
    "\n",
    "# Translate nominal target values into numeric values\n",
    "for ii, tgt in enumerate(targets):\n",
    "    if tgt == 'entailment':\n",
    "        targets[ii] = 2\n",
    "    elif tgt == 'not_entailment':\n",
    "        targets[ii] = 1\n",
    "    else:\n",
    "        print('Unknown target label.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data-Tokenization-Test\"></a>\n",
    "<h2> Tokenize the Test Data and Upload as a SAS Viya Table </h2>\n",
    "\n",
    "This process follows the same procedure that was used with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: 10% of the observations tokenized.\n",
      "NOTE: 20% of the observations tokenized.\n",
      "NOTE: 30% of the observations tokenized.\n",
      "NOTE: 40% of the observations tokenized.\n",
      "NOTE: 50% of the observations tokenized.\n",
      "NOTE: 60% of the observations tokenized.\n",
      "NOTE: 70% of the observations tokenized.\n",
      "NOTE: 80% of the observations tokenized.\n",
      "NOTE: 90% of the observations tokenized.\n",
      "NOTE: 100% of the observations tokenized.\n",
      "NOTE: all observations tokenized.\n",
      "\n",
      "WARNING: 10 out of 5266 observations exceeded the maximum sequence length\n",
      "These observations have been truncated so that only the first 256 tokens are used.\n",
      "\n",
      "NOTE: uploading data to table bert_data.\n",
      "NOTE: there are 5266 observations in the data set.\n",
      "\n",
      "NOTE: data set ready.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify parameters for the bert_prepare_data function\n",
    "num_tgt_var, test = bert_prepare_data(s, \n",
    "                                      bert.get_tokenizer(), \n",
    "                                      bert.get_max_sequence_length(), \n",
    "                                      input_a=input_a, \n",
    "                                      input_b=input_b, \n",
    "                                      target=targets, \n",
    "                                      classification_problem=bert.get_problem_type(), \n",
    "                                      segment_vocab_size=bert.get_segment_size(), \n",
    "                                      verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Inference-Test-Data\"></a>\n",
    "<h2> Perform Inference Using the Test Data </h2>\n",
    "\n",
    "Next, the example uses the ``predict()`` member function to perform inference.  Once more, the code uses the ``get_text_parameters()`` function to set the text parameters appropriately.\n",
    "\n",
    "The results below show the misclassification rate and the loss error.  The correct classification rate is simply (1-0.965)\\*100 = 90.35%. The example's 90.35% classification rate is slightly better than the 90.1% classification rate achieved for the BERT<sub>base</sub> model in *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*.  \n",
    "\n",
    "Remember that the misclassification and loss error results for this example were obtained using the **dev** data set, whereas the results shown in the original academic paper were obtained using the **test** data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Descr         Value\n",
      "0  Number of Observations Read          5266\n",
      "1  Number of Observations Used          5266\n",
      "2  Misclassification Error (%)      9.646791\n",
      "3                   Loss Error      0.300679\n"
     ]
    }
   ],
   "source": [
    "# Score the BERT dev data\n",
    "res = bert.predict(test, \n",
    "                   text_parms=bert.get_text_parameters()\n",
    "                   )\n",
    "\n",
    "print(res['ScoreInfo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Terminate the SAS Viya Session </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.000162s</span> &#183; <span class=\"cas-user\">user 0.000151s</span> &#183; <span class=\"cas-memory\">mem 0.207MB</span></small></p>"
      ],
      "text/plain": [
       "+ Elapsed: 0.000162s, user: 0.000151s, mem: 0.207mb"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.endsession()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
